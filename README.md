# Dbt--Snowflake--Airflow-Data-Pipeline
Building an ELT Pipeline with dbt, Snowflake, and Airflow

Project Overview:
This project demonstrates how to build an ELT pipeline using dbt, Snowflake, and Airflow. Follow the steps below to set up your environment, configure dbt, create models, macros, tests, and deploy on Airflow.

Steps:
Step 1: Setup Snowflake Environment
Step 2: Configure dbt Profile
Step 3: Create Source and Staging Files
Step 4: Macros (D.R.Y.)
step 5: Step 5: Transform Models (Fact Tables, Data Marts)
Step 6: Generic and Singular Tests
Step 7: Deploy on Airflow


Successful deployment in Airflow(using astronomer-cosmos)

![image](https://github.com/user-attachments/assets/2d58e5b7-d372-47f6-8a61-a84801465e56)
