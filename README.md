# Dbt-Snowflake-Airflow-ELT Data-Pipeline
Building an ELT Pipeline with dbt, Snowflake, and Airflow

Project Overview:
This project demonstrates how to build an ELT pipeline using dbt, Snowflake, and Airflow. Follow the steps below to set up your environment, configure dbt, create models, macros, tests, and deploy on Airflow.

Steps:

Step 1: Setup Snowflake Environment

Step 2: Configure dbt Profile

Step 3: Create Source and Staging Files

Step 4: Macros (D.R.Y.)

step 5: Step 5: Transform Models (Fact Tables, Data Marts)

![image](https://github.com/user-attachments/assets/8a22a424-5df4-484e-b306-cfb488556d0b)


Step 6: Generic and Singular Tests

![image](https://github.com/user-attachments/assets/01868363-65e4-4af3-b622-74e7ba621ca7)


Step 7: Deploy on Airflow


Successful deployment in Airflow(using astronomer-cosmos)

![image](https://github.com/user-attachments/assets/2d58e5b7-d372-47f6-8a61-a84801465e56)
